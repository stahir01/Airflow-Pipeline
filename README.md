# Airflow-Docker

## Setting up Airflow with Docker

This section provides instructions for setting up Apache Airflow using Docker Compose.  This setup assumes you have Docker and Docker Compose installed on your system.

### Prerequisites

* **Docker:** [Install Docker](https://docs.docker.com/get-docker/)
* **Docker Compose:** [Install Docker Compose](https://docs.docker.com/compose/install/)

### Setup Instructions

1.  **Install Python 3.10 Environment (Optional, for running scripts):**

    * If you need a Python 3.10 environment for running Airflow scripts or other utilities, you can create one using the following bash commands:

        ```bash
        python3 -m venv airflow_env
        ```

2.  **Activate the Python Environment (Optional):**

    * To activate the virtual environment you created, use:

        ```bash
        source airflow_env/bin/activate
        ```
        *(Note: The path might be `scripts/bin/activate` in some systems.)*

3.  **Install Python Dependencies:**

    * Install the required Python libraries for Airflow (and any other project dependencies) using `pip` and the `requirements.txt` file.  This will ensure you have all the necessary packages.  If you activated the virtual environment in the previous step, make sure you are still in the environment when running this command.

        ```bash
        pip install -r requirements.txt
        ```
4. **Enable Email Notifications:**
    * To receive email alerts for DAG task failures, you need to configure email notifications in Airflow.
    * First, enable email notifications in your Google account by following the instructions in this guide: [Email Alerting with Airflow](https://medium.com/@chibuokejuliet/email-alerting-with-airflow-c0a5a1f413b4).
    * Add the following variables to your `.env` file:

        ```
        AIRFLOW_RECOVERY_EMAIL=<your_email@gmail.com>
        AIRFLOW_RECOVERY_PASSWORD=<your_email_app_generated_password>
        ```

5.  **Build the Docker Images:**

    * This command builds the Docker images defined in the `docker-compose.yml` file. This includes the Airflow image and any other services it depends on (like the database).  This step is crucial for setting up the Airflow environment.

        ```bash
        docker-compose build
        ```

6.  **Run the Docker Containers:**

    * This command starts all the services defined in the `docker-compose.yml` file.  Airflow and its related services will be running in Docker containers.

        ```bash
        docker-compose up
        ```

7.  **Access Airflow:**

    * Once the containers are running, you can access the Airflow web interface in your browser at:

        ```
        http://localhost:8080
        ```

    * You will be prompted for a username and password.  The default username and password for Airflow are usually `airflow` and `airflow`.

### Airflow Structure
This project employs a modular structure to organize Airflow components, enhancing maintainability and scalability. Here's a breakdown of the main directories:

```
airflow
├── config
├── dags
│   ├── project1
│   │   ├── dag1.py
│   │   ├── dag2.py
│   │   └── helper/
│   ├── project2/
│   ├── project3/
│   └── utils/
├── datasets/
├── logs/
├── plugins/
├── secrets/
└── .env
```


* `dags`: This directory contains the Directed Acyclic Graphs (DAGs), which define your workflows.
    * Modular Structure: DAGs are further organized into project-specific subdirectories (e.g., `project1`, `project2`). This allows for better management of DAGs as the project grows.
        * `project1`: Contains DAGs (`dag1.py`, `dag2.py`) and project-specific helper functions (`helper/`).
        * `project2`, `project3`: Similar structure for other projects.
        * `utils`: Contains utility functions that can be used across different projects.
    * `__init__.py`:  This file in each directory (e.g., `dags`, `dags/project1`) enables Python's modular import system.  This allows you to import DAGs and helper functions from other directories, promoting code reuse and a cleaner structure.
* `config`:  This directory holds configuration files for Airflow, such as settings for connections, pools, and other Airflow configurations.
* `datasets`: This directory is used to store temporary results or intermediate datasets generated by Airflow tasks. This provides a dedicated space for data produced during workflow execution.
* `logs`:  This directory contains Airflow logs, which are essential for monitoring and debugging your workflows.
* `plugins`:  This directory is for custom Airflow plugins, which can extend Airflow's functionality with custom operators, sensors, hooks, and more.
* `secrets`:  This directory is used to store sensitive information, such as API keys, database passwords, and other credentials.  **Note:** Exercise caution when handling files in this directory and ensure appropriate security measures are in place.  Ideally, use Airflow's built-in secrets management or a dedicated secrets management solution.
* `.env`: This file is used to store environment variables, which can be used to configure Airflow and your DAGs.  This is useful for settings that vary between environments (e.g., development, production).

**Note:** If you want to add other folders to your Airflow setup, you'll need to include them in the `volumes` section of your `docker-compose.yml` file to ensure they are properly mounted and accessible by the Airflow containers.


### DAG: Website Documentation Crawler Explanation
The DAG for the website documentation crawler is designed to automate the process of crawling a website, extracting content, and saving it in a structured format.  Below is an explanation of the key components and logic of the DAG.

This project implements a web crawler designed to extract content from website documentation and save it in a structured format. It is built for integration with Apache Airflow.


#### Key Components and How They Work

1.  **`process_website` Function:**

    * Orchestrates the crawling process.
    * Takes crawler configuration (`base_url`, `project_name`, `max_depth`, `max_queue_size`, `timeout`, `headers`) as input.
    * Uses a queue (`deque`) to manage URLs to be visited.
    * Iteratively fetches page content, extracts relevant information, saves it as Markdown, and adds new links to the queue.
    * Crawling continues until the queue is empty or the maximum depth is reached.

2.  **`scrape_page` Function:**

    * Scrapes content from a single web page.
    * Takes a URL and crawler configuration as input.
    * Fetches the page, parses HTML, extracts title, main content, and links.
    * Returns the extracted data as a dictionary.
    * Uses `BeautifulSoup` for HTML parsing and `markdownify` for HTML-to-Markdown conversion.

3.  **`save_as_markdown` Function:**

    * Saves extracted content to a Markdown file.
    * Takes the content dictionary and project name as input.
    * Generates a filename, creates the directory structure, and writes the Markdown content to the file.

4.  **`get_links` Function:**

    * Extracts all internal links from a BeautifulSoup object
    * Takes a BeautifulSoup object and base URL as input
    * Finds all hyperlinks and returns a list of absolute URLs

5.  **Airflow DAG (`website_docs_crawler`)**

    * Defines the web crawling workflow.
    * Uses a `PythonOperator` to execute the `process_website` function.
    * DAG is configured to run daily.
    * `crawl_website_task` performs the crawling.

#### Explanation of Key Parameters

* `max_depth`: Controls how far the crawler follows links from the starting URL.
    * 0: Only the starting URL is visited.
    * 1:  The starting URL and its direct links are visited.
    * 2:  The starting URL, its direct links, and their links are visited, and so on.
* `max_queue_size`: Limits the number of URLs the crawler keeps in its queue, preventing excessive memory usage.


### Website Documentation Crawler

#### DAG Structure
The Airflow DAG(s) for this crawler are located within the `dags/docs_crawler` directory.


#### Task Details

* **Single Task Design**: The current DAG implementation uses a single task. Due to the recursive nature of web crawling, where the crawler discovers new pages by following links, it's challenging to decompose the core crawling logic into multiple, independent Airflow tasks in a traditional way. Airflow DAGs are designed around well-defined sequences of tasks, and recursion doesn't fit that model neatly.

* **Typical Workflow Context**: In a larger data pipeline, this crawling task would likely be one component within a broader workflow. Common subsequent steps include:
    * **ETL (Extract, Transform, Load):** Further processing of the extracted data.
    * **Data Conversion:** Converting the Markdown files into other formats (e.g., JSON).
    * **Vector Database Ingestion:** Storing the processed data in a vector database to support Retrieval Augmented Generation (RAG) applications.

* **Helper Function Approach**: The crawling logic is designed to be encapsulated as a reusable helper function. You can find a similar approach in `dags/docs_crawler/helper/scraper.py` (though it's noted as incomplete). The goal is to create a modular function that can be easily integrated into various Airflow DAGs as a task. The `scraper.py` example aims to output data in JSON format.

#### Potential Improvements

The following are potential improvements that could not be implemented in the current code due to time constraints:

* **Unit Testing**: Adding unit tests using `pytest` to ensure the reliability and correctness of the crawling logic.
* **JSON Output**: Storing the extracted data in JSON format for better data interoperability and compatibility with other systems.
* **Database Storage**: Storing the crawled data in a database such as MongoDB. Storing data locally in Markdown files is not efficient for large-scale applications.
* **External Library - crawl4ai**: During the development of this crawler, the `crawl4ai` library was identified as a potentially more efficient and robust solution. `crawl4ai` is specifically designed for web crawling and offers features such as:
    * **Asynchronous Crawling**: This allows for faster and more efficient crawling by making multiple requests concurrently.
    * **Built-in Data Storage**: It can store crawled data in various formats, including JSON and databases.
    * **BFS (Breadth-First Search) and DFS (Depth-First Search)**: The library supports both BFS and DFS crawling strategies, allowing for flexibility in how the crawler navigates through web pages.

For real-world production applications, `crawl4ai` (or a similar specialized library) is recommended due to its efficiency and features tailored for RAG.
