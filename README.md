# Airflow-Docker

## Setting up Airflow with Docker

This section provides instructions for setting up Apache Airflow using Docker Compose.  This setup assumes you have Docker and Docker Compose installed on your system.

### Prerequisites

* **Docker:** [Install Docker](https://docs.docker.com/get-docker/)
* **Docker Compose:** [Install Docker Compose](https://docs.docker.com/compose/install/)

### Setup Instructions

1.  **Install Python 3.10 Environment (Optional, for running scripts):**

    * If you need a Python 3.10 environment for running Airflow scripts or other utilities, you can create one using the following bash commands:

        ```bash
        python3 -m venv airflow_env
        ```

2.  **Activate the Python Environment (Optional):**

    * To activate the virtual environment you created, use:

        ```bash
        source airflow_env/bin/activate
        ```
        *(Note: The path might be `scripts/bin/activate` in some systems.)*

3.  **Install Python Dependencies:**

    * Install the required Python libraries for Airflow (and any other project dependencies) using `pip` and the `requirements.txt` file.  This will ensure you have all the necessary packages.  If you activated the virtual environment in the previous step, make sure you are still in the environment when running this command.

        ```bash
        pip install -r requirements.txt
        ```
4. **Enable Email Notifications:**
    * To receive email alerts for DAG task failures, you need to configure email notifications in Airflow.
    * First, enable email notifications in your Google account by following the instructions in this guide: [Email Alerting with Airflow](https://medium.com/@chibuokejuliet/email-alerting-with-airflow-c0a5a1f413b4).
    * Add the following variables to your `.env` file:

        ```
        AIRFLOW_RECOVERY_EMAIL=<your_email@gmail.com>
        AIRFLOW_RECOVERY_PASSWORD=<your_email_app_generated_password>
        ```

5.  **Build the Docker Images:**

    * This command builds the Docker images defined in the `docker-compose.yml` file. This includes the Airflow image and any other services it depends on (like the database).  This step is crucial for setting up the Airflow environment.

        ```bash
        docker-compose build
        ```

6.  **Run the Docker Containers:**

    * This command starts all the services defined in the `docker-compose.yml` file.  Airflow and its related services will be running in Docker containers.

        ```bash
        docker-compose up
        ```

7.  **Access Airflow:**

    * Once the containers are running, you can access the Airflow web interface in your browser at:

        ```
        http://localhost:8080
        ```

    * You will be prompted for a username and password.  The default username and password for Airflow are usually `airflow` and `airflow`.

### Airflow Structure
This project employs a modular structure to organize Airflow components, enhancing maintainability and scalability. Here's a breakdown of the main directories:

```
airflow
├── config
├── dags
│   ├── project1
│   │   ├── dag1.py
│   │   ├── dag2.py
│   │   └── helper/
│   ├── project2/
│   ├── project3/
│   └── utils/
├── datasets/
├── logs/
├── plugins/
├── secrets/
└── .env
```


* `dags`: This directory contains the Directed Acyclic Graphs (DAGs), which define your workflows.
    * Modular Structure: DAGs are further organized into project-specific subdirectories (e.g., `project1`, `project2`). This allows for better management of DAGs as the project grows.
        * `project1`: Contains DAGs (`dag1.py`, `dag2.py`) and project-specific helper functions (`helper/`).
        * `project2`, `project3`: Similar structure for other projects.
        * `utils`: Contains utility functions that can be used across different projects.
    * `__init__.py`:  This file in each directory (e.g., `dags`, `dags/project1`) enables Python's modular import system.  This allows you to import DAGs and helper functions from other directories, promoting code reuse and a cleaner structure.
* `config`:  This directory holds configuration files for Airflow, such as settings for connections, pools, and other Airflow configurations.
* `datasets`: This directory is used to store temporary results or intermediate datasets generated by Airflow tasks. This provides a dedicated space for data produced during workflow execution.
* `logs`:  This directory contains Airflow logs, which are essential for monitoring and debugging your workflows.
* `plugins`:  This directory is for custom Airflow plugins, which can extend Airflow's functionality with custom operators, sensors, hooks, and more.
* `secrets`:  This directory is used to store sensitive information, such as API keys, database passwords, and other credentials.  **Note:** Exercise caution when handling files in this directory and ensure appropriate security measures are in place.  Ideally, use Airflow's built-in secrets management or a dedicated secrets management solution.
* `.env`: This file is used to store environment variables, which can be used to configure Airflow and your DAGs.  This is useful for settings that vary between environments (e.g., development, production).

**Note:** If you want to add other folders to your Airflow setup, you'll need to include them in the `volumes` section of your `docker-compose.yml` file to ensure they are properly mounted and accessible by the Airflow containers.
